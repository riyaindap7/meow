{
  "file_path": "uploads/1763396333588.pdf",
  "filename": "DocAgent (1).pdf",
  "mime_type": "application/pdf",
  "full_text": "DocAgent: A Multi-Agent System for Automated Code Documentation\nGeneration\nDayuYang* AntoineSimoulin\u2020 XinQian\u2020 XiaoyiLiu\u2020 YuweiCao\u2020 ZhaopuTeng\u2020 GreyYang\nMetaAI\n{dayuyang,antoinesimoulin,xinqian,xiaoyiliu,yuweicao,zhaoputeng,glyang}@meta.com\nAbstract lags behind code changes (Aghajani et al., 2019;\nRobillard,2009;Uddinetal.,2021).\nHigh-qualitycodedocumentationiscrucialfor\nWhile LLM-based solutions\u2014such as Fill-in-\nsoftwaredevelopmentespeciallyintheeraof\nthe-Middle(FIM)predictors(Roziereetal.,2023;\nAI. However, generating it automatically us-\ningLargeLanguageModels(LLMs)remains GitHub,2024)andchatagents(Meta,2025;Ope-\nchallenging,asexistingapproachesoftenpro- nAI, 2022)\u2014offer automation, extensive stud-\nduceincomplete,unhelpful,orfactuallyincor- ies (Dvivedi et al., 2024; Zhang et al., 2024; Zan\nrectoutputs. WeintroduceDocAgent,anovel\net al., 2022; Zheng et al., 2024), along with our\nmulti-agentcollaborativesystemusingtopolog-\nempiricalanalyses(\u00a74),revealthreerecurringlim-\nical code processing for incremental context\nitations. First,theseapproachesoftenomitessen-\nbuilding. Specializedagents(Reader,Searcher,\ntial information (e.g., parameter or return-value\nWriter, Verifier, Orchestrator)thencollabora-\ntively generate documentation. We also pro- descriptions). Second,theytypicallyofferminimal\nposeamulti-facetedevaluationframeworkas- contextorrationale,limitingtheusefulnessofthe\nsessingCompleteness,Helpfulness,andTruth- generateddocumentation. Third,theysometimes\nfulness. Comprehensive experiments show hallucinatenon-existentcomponents,especiallyin\nDocAgentsignificantlyoutperformsbaselines\nlargeorproprietaryrepositories,underminingfac-\nconsistently. Ourablationstudyconfirmsthe\ntualcorrectness(Zanetal.,2022;Maetal.,2024;\nvital role of the topological processing order.\nAbeduetal.,2024).\nDocAgentoffersarobustapproachforreliable\nWeidentifythreeprimarychallengesthatdrive\ncodedocumentationgenerationincomplexand\nproprietaryrepositories. Ourcode1andvideo2 these shortcomings. (1) Context Identification\narepubliclyavailable. andRetrieval: Large,complexrepositoriesmake\nitnon-trivialtopinpointwhichfiles,dependencies,\n1 Introduction\nor external references are genuinely relevant for\nHigh-quality code documentation is essential for agivencomponent. (2)NavigatingComplexDe-\neffective software development (De Souza et al., pendencies: Codebasesoftenexhibitdependency\n2005;Garousietal.,2015;ChenandHuang,2009), chainsthatexceedtypicalLLMcontextlimits,re-\nandhasbecomeincreasinglyimportantasAImod- quiringstrategiccontextmanagement. (3)Robust\nelsdependonaccuratedocstrings3 forcodecom- andScalableEvaluation: Existingevaluationmet-\nprehension tasks (Zhou et al., 2022; Yang et al., ricslikeBLEUorROUGE(Royetal.,2021;Guel-\n2024; Anthropic, 2025). However, creating and man et al., 2024) incompletely capture the multi-\nmaintainingdocumentationislabor-intensiveand facetedgoalsofdocumentation,whilehumaneval-\nprone to errors (McBurney et al., 2017; Parnas, uation,thoughmorereliable,isexpensiveandsub-\n2010). Eventop-starredopen-sourcerepositories jective(Luoetal.,2024).\nonGitHuboftenexhibitlowdocstringcoverageand Totacklethesechallenges,weintroduceDocA-\nquality,4 leadingtodocumentationthatfrequently gent, a multi-agent system that processes code\ninatopologicallysortedorderandleveragesspe-\n*CorrespondingAuthor.\n\u2020Equalcontribution. cializedagents(Reader, Searcher, Writer, Ver-\n1https://github.com/facebookresearch/DocAgent ifier, Orchestrator) to collaboratively generate\n2https://youtu.be/e9IjObGe9_I\ndocumentation. This mimics human workflows\n3We use \"code documentation\" and \"docstring\" inter-\nandmanagescontexteffectively. Wealsopropose\nchangeablythroughoutthepaper.\n4SeeAppendixCformoredetails. an automatic and robust multi-faceted evaluation\n1\n5202\nyaM\n32\n]ES.sc[\n3v52780.4052:viXra\nFigure1: ArchitectureofDocAgent: (1)TheNavigatorModuleusesASTparsingforaDependencyDAGand\ntopologicaltraversal. (2)TheMulti-Agentframeworkusesspecializedagents(Reader,Searcher,Writer,Verifier)\nwithtoolsforcontext-awaredocumentationgeneration.\nframeworkassessingCompleteness,Helpfulness, terdependencies. Thesedependenciesincludefunc-\nand Truthfulness via deterministic checks and tion/method calls, class inheritance, attribute ac-\nLLM-as-judge. Our main contributions are: 1) cess,andmoduleimports. Thesecomponentsand\nDocAgent,Amulti-agent,topologicallystructured relationshipsareusedtoconstructadirectedgraph\nsystem for context-aware documentation genera- wherenodesrepresentcodecomponentsandadi-\ntion. 2)Arobustevaluationframeworkmeasuring rectededgefromAtoBsignifiesthatAdepends\ncompleteness,helpfulness,andfactualconsistency onB(A\u2192B).Toenabletopologicalsorting, cy-\nofcodedocumentation. 3)Comprehensiveexperi- cles within the graph are detected using Tarjan\u2019s\nmentsondiverserepositoriesshowDocAgentcon- algorithm(Tarjan,1972)andcondensedintoasin-\nsistentlyoutperformsstate-of-the-artbaselines. glesupernode. ThisresultsinaDirectedAcyclic\nGraph(DAG)representingtherepository\u2019sdepen-\n2 Methodology\ndencystructure.\nTheprocessbeginswithstaticanalysisoftheen-\nDocAgentoperatesintwostagestohandlecomplex\ntiretargetrepository. AbstractSyntaxTrees(ASTs)\ndependenciesandensurecontextrelevance. First,\nareparsedforallsourcefilestoidentifycorecode\ntheNavigatordeterminesanoptimal,dependency-\ncomponents(e.g.,functions,methods,classes)and\naware processing order (\u00a72.1). Second, a Multi-\ntheir interdependencies. These dependencies en-\nAgentSystemincrementallygeneratesdocumenta-\ncompass function/method calls, class inheritance\ntion,leveragingspecializedagentsforcodeanaly-\nrelationships, attribute accesses, and module im-\nsis,informationretrieval,drafting,andverification\nports. Based on this analysis, a directed graph is\n(\u00a72.2). Figure1illustratesthisarchitecture.\nconstructed where nodes represent code compo-\n2.1 Navigator: Dependency-AwareOrder nents and a directed edge from component A to\nGeneratingaccuratedocumentationoftenrequires componentB(A\u2192B)signifiesthatAdependson\nunderstandingitsdependencies. However,naively B(i.e.,Bmustbeunderstoodtofullyunderstand\nincludingthefullcontextofalldirectandtransitive\nA)5.\ndependencies can easily exceed context window Topological Traversal for Hierarchical Gener-\nlimitespeciallyinlarge,complexrepositories. To ation. Using the DAG, the Navigator performs a\naddress this, the Navigator module establishes a topological sort to determine the documentation\nprocessingorderthatensurescomponentsaredoc- generationorder. Thetraversaladherestothe\"De-\numentedonlyaftertheirdependencieshavebeen pendenciesFirst\"principles: Acomponentispro-\nprocessed, thereby enabling incremental context cessedonlyafterallcomponentsitdirectlydepends\nbuilding. onhavebeendocumented6. Thistopologicalorder-\nDependency Graph Construction. DocAgent ingensuresthat,bythetimethemulti-agentsystem\nfirst performs static analysis on the entire target generates documentation for a given component,\nrepository. It parses the Abstract Syntax Trees\n5CycleswithinthegrapharedetectedusingTarjan\u2019salgo-\n(ASTs) of source files to identify code compo-\nrithm(Tarjan,1972)andcondensedintoasinglenode.\nnents (functions, methods, classes) and their in- 6Methodsaredocumentedbeforetheirenclosingclass.\n2\nthereforerevealthepurposeofthefocalcodecom-\nponent. This is particularly important for public\nfunctionsorAPIsexposedtotheusersoftherepos-\nitory.\nExternalrequeststargetinformationnotdirectly\npresentorinferablefromthecodebaseitself,such\nasdomain-specificknowledgeorthird-partylibrary\nfunctionalities(seeAppendixB).\nSearcher. The Searcher agent is responsible\nFigure2: ScreenshotofDocAgentlivecodedocumen- forfulfillingtheReader\u2019sinformationrequestsus-\ntationgenerationpage. ingspecializedtools: InternalCodeAnalysisTool:\nThis tool leverages static analysis capabilities to\nallofitsdependencieshavealreadybeendescribed.\nnavigate the codebase. It can retrieve the source\nTherefore,eachcodedocumentationonlyneedsthe\ncode and existing documentation of specified in-\ninformationofitsone-hopdependencies,eliminat-\nternal components, identify call sites for the fo-\ning the need to pull in an ever-growing chain of\ncalcomponent,tracedependenciesusingthepre-\nbackgroundinformation.\ncomputedgraphoron-the-flyanalysis,andextract\nrelevant structural information (e.g., class hierar-\n2.2 Multi-AgentDocumentationGeneration\nchies, method signatures). External Knowledge\nFollowingNavigator\u2019sorder,themulti-agentsys- Retrieval Tool: This tool interfaces with external\ntemgeneratesdocumentationforeachcomponent knowledgesourcesviaagenericretrievalAPI.It\nusing four specialized agents coordinated by an formulatesqueriesbasedontheReader\u2019srequests\nOrchestrator. Inputisthefocalcomponent\u2019ssource forexternalconceptsandprocessestheresultsto\ncodeincludingnewlygenerateddocumentation. extract pertinent explanations, definitions, or de-\nReader. TheReaderagentinitiatestheprocess scriptions.\nbyanalyzingthefocalcomponent\u2019scode. Itspri- TheSearcherconsolidatestheretrievedinternal\nmarygoalistodeterminetheinformationrequired code information and external knowledge into a\ntogenerateacomprehensiveandhelpfulcodedocu- structuredformat,whichservesasthecontextfor\nmentation. Itassessesthecomponent\u2019scomplexity, thesubsequentagents.\nvisibility(public/private),andimplementationde- Liketwohumanagentscollaborateonaproject\ntailstodecide: Ifadditionalcontextisneeded: Sim- andtalkwitheachother,afterSearchersendthere-\nple,self-containedcomponentsmightnotrequire trievedinformationbacktothereader,readerread\nexternalinformation. Whatcontextisneeded: This theupdatedcontextandthefocalcodecomponent,\ninvolvesidentifyingspecificinternaldependencies and see if the context is adequate for generating\n(functions/classesituses),usagecontexts(where thedocumenation. Ifreaderstillfeeltheretrieved\nthecomponentiscalled,revealingitspurpose),or contextisstillnotadequate,readercanfurthersend\nexternal concepts (algorithms, libraries, domain information request to the searcher. So the infor-\nknowledge)referencedimplicitlyorexplicitly. mation request, and new information can be sent\nTheagentoutputsstructuredXMLrequestsfor backandforthbetweenreaderandsearcher,until\ntwo types of information requests (1) internal in- adequateinformationisretrieved.\nformationaboutrelatedcodecomponents,and(2) Writer. The Writer agent receives the focal\nexternalknowledgeforspecializedalgorithmsor component\u2019scodeandthestructuredcontextcom-\ntechniques. piled by the Searcher. Its task is to generate the\nThe internal information request consists with code documentation. The generation process is\nthe dependency and the reference. Dependency guidedbypromptsthatspecifythedesiredstructure\nmeansthefocalcomponentcallsothercomponents and content based on the component type: Func-\ndefinedintherepository,wherereaderwilldeter- tions/Methods: Typically require a summary, ex-\nminateifadependentisneededornottoprovide tendeddescription,parameterdescriptions(Args),\nnecessarycontextinformation. return value description (Returns), raised excep-\nReferencemeansthefocalcomponentiscalled tions(Raises),andpotentiallyusageexamples(es-\ninsomewhereinthecoderepository,showinghow pecially for public-facing components). Classes:\nit can be used in the real-world application and Typicallyrequireasummary,extendeddescription,\n3\ninitializationexamples,constructorparameterde-\nscriptions(Args),andpublicattributedescriptions\n(Attributes).\nThe Writer synthesizes information from both\nthe code and the provided context to produce a\ndraftcodedocumentationadheringtotheserequire-\nments.\nVerifier. TheVerifiertakethecontext,codecom-\nponent, and generated code documentation from\nthewriterasinputs,evaluatesthequalityofcode\ndocumentationagainstpredefinedcriteria: informa-\ntion value, detail level, and completeness. Upon\nevaluation, the Verifier either approves the docu-\nmentation or provides specific improvement sug-\ngestionsthroughstructuredfeedback.\nVerifier can talk to writer if the issue can be Figure 3: Multi-facet Evaluation Framework of code\naddresswithoutadditionalcontextinformation,for documentation, assessing quality along three dimen-\nsions: (1)Completenessmeasuresstructuraladherence\nexample: formatissue,whichcanbeeasilyaddress\ntodocumentationconventions; (2)Helpfulnessevalu-\nbyaskingwritertorewrite.\natespracticalutility;and(3)Truthfulnessverifiesfactual\nIf the issue is relevant to lack of information, accuracy.\nandadditionalcontextisneeded,veirfiercanalso\nprovidesuggestiontoreader,andadditionalinfor-\n3 EvaluationFramework\nmation will be gathered through another Reader-\nSearchercycle.\nOrchestrator. An Orchestrator manages the\nEvaluatingthequalityofautomaticallygenerated\nagentworkflowthroughaniterativeprocess. The\ncode documentation is challenging. Traditional\ncycle begins with the Reader analyzing the focal\nmetrics commonly used in natural language gen-\ncomponentandrequestingnecessarycontext. The\neration,suchasBLEUorROUGEcannotbeused\nSearchergathersthisinformation,afterwhichthe\nbecauseoflackofgoldreferences(Royetal.,2021;\nWriter generates a docstring. The Verifier then\nGuelmanetal.,2024). Simpleheuristicslikedocu-\nevaluatesthedocstringquality,eitherapprovingit\nmentationlengthareinsufficientindicatorsofac-\norreturningitforrevision. Thisprocesscontinues\ntualutility. Whilehumanevaluationprovidesthe\nuntilasatisfactorycodedocumentaionisgenerated\nmost accurate assessment (Luo et al., 2024), it is\noramaximumiterationlimitisreached.\ninherently subjective, expensive, and difficult to\nAdaptive Context Management: To handle po-\nscale,renderingitimpracticalforlarge-scaleexper-\ntentially large contexts retrieved by the Searcher,\nimentsorcontinuousintegrationscenarios.\nespecially for complex components, the Orches-\ntrator implements an adaptive context truncation To overcome these limitations, we propose a\nmechanism. It monitors the total token count of comprehensiveandscalableevaluationframework\nthe context provided to the Writer. If the context designed to systematically assess documentation\nexceedsaconfigurablethreshold(basedontheun- qualityalongthreecrucialdimensions: Complete-\nderlyingLLM\u2019slimits),theOrchestratorappliesa ness, Helpfulness, and Truthfulness. This multi-\ntargetedtruncationstrategy. Itidentifiesthelargest facetedapproachcombinesdeterministicstructural\nsectionswithinthestructuredcontext(e.g.,external checks, LLM-based qualitative assessments, and\nknowledgesnippets,specificdependencydetails) fact-checkingagainstthecodebaseitself,providing\nand selectively removes content from the end of a holistic view of the generated documentation\u2019s\nthesesectionstoreducethetokencountwhilepre- value. Ourmethodologyisinformedbyestablished\nservingtheoverallstructure. Thisensuresthatthe softwareengineeringbestpracticesfordocumen-\ncontextremainswithinoperationallimits,balanc- tationandaddressesthespecificshortcomingsob-\ningcontextualrichnesswithmodelconstraints. servedinexistingLLM-basedgenerationsystems.\n4\n3.2 Helpfulness\nHelpfulnessassessesthesemanticqualityandprac-\nticalutilityofthedocumentationcontent. Ahelpful\ndocstring goes beyond merely restating code ele-\nments;itelucidatesthepurpose,usagecontext,de-\nsignrationale,andpotentialconstraintsofthecode.\nKeyaspectsinclude: ClarityandConciseness: Is\nthe summary informative yet brief? Descriptive\nDepth: Doestheextendeddescriptionprovidesuf-\nficientcontext,explainthe\u2019why\u2019behindthecode,\nFigure 4: Screenshot of DocAgent Live Evaluation\nFramework or mention relevant scenarios or edge cases? Pa-\nrameter/Attribute Utility: Are descriptions for\ninputs and attributes meaningful, specifying ex-\n3.1 Completeness\npected types, value ranges, or constraints, rather\nthanjustechoingnames? Guidance: Doesthedoc-\nCompletenessmeasurestheextenttowhichthegen-\numentationeffectivelyguideadeveloperonwhen\nerateddocumentationadherestostandardstructural\nandhowtousethecomponent?\nconventionsandincludesessentialcomponentsex-\npected for a given code element (e.g., function, Assessingthesequalitativeaspectsautomatically\nclass). High-qualitycodedocumentationtypically ischallenging. Inspiredbyrecentworkonevalu-\nincludesnotonlyasummarybutalsodescriptions atingcomplexgenerationtasks(Wangetal.,2024;\nofparameters,returnvalues,raisedexceptions,and Zhugeetal.,2024),weutilizeanLLM-as-judgeap-\npotentiallyusageexamples,dynamicallydepend- proach,carefullystructuredtoenhancerobustness\ningontheelement\u2019ssignature,bodyandvisibility. andconsistency. Tomitigatepotentialbiasesand\nvariabilityassociatedwithLLMjudgments,weim-\nTo quantify completeness, we employ an au-\nplementasophisticatedframework: Component-\ntomated checker based on Abstract Syntax Tree\nSpecific Evaluation: We decompose the evalu-\n(AST)analysisandregularexpressions. Thepro-\nation by assessing distinct parts of the docstring\ncessinvolves: ASTParsing: Identifyingcodecom-\nseparately(e.g.,summary,maindescription,param-\nponents(classes,functions,methods)andextract-\neterdescriptions)usingtailoredpromptsforeach.\ning their generated docstrings. Code Analysis:\nStructured Prompt Engineering: Each prompt\nAnalyzingthecodesignatureandbody(e.g.,pres-\nincludes: 1) Explicit Scoring Rubrics: Detailed\nenceofparameters,returnstatements,raisestate-\ncriteriafora5-pointLikertscale(1=Poorto5=Ex-\nments)andvisibility(public/private)todetermine\ncellent),definingexpectationsforeachscorelevel\ntherequired documentationsectionsdynamically.\nregardingclarity,depth,andutility. 2)Illustrative\nFor instance, a function without parameters does\nExamples: Concrete examples of good and bad\nnotrequirean\"Args\"section,whileapublicclass\ndocumentationsnippetscorrespondingtodifferent\nmethodmightbenefitmorefroman\"Example\"sec-\nscorelevels,groundingtheevaluationcriteria. 3)\ntionthanaprivatehelperfunction. SectionIden-\nStep-by-StepInstructions: GuidingtheLLMtoan-\ntification: Detectingthepresenceofstandardsec-\nalyzethecode,comparethedocstringagainstthe\ntions(e.g.,Summary,Description,Args,Returns,\nrubric,considerthecode\u2019scontext,andjustifyits\nRaises,Examples,Attributesforclasses)withinthe\nrating. 4)StandardizedOutputFormat: Requiring\ndocstringusingpredefinedpatternsandstructural\ntheLLMtoprovidestructuredoutput,includingde-\ncues. Scoring: Calculatingacompletenessscore\ntailedreasoning,specificsuggestionsforimprove-\nfor each docstring as the proportion of required\nment(ifapplicable),andthefinalnumericalscore.\nsectionsthatarepresent. Thisyieldsanormalized\nThisfacilitatesanalysisandconsistencychecking.\nscorebetween0.0and1.0.\nThisdeterministicapproachprovidesanobjec- This structured approach allows for scalable\ntive measure of structural adherence, indicating assessment of semantic quality, moving beyond\nwhetherthedocumentationmeetsbasicformalre- surface-levelcheckstogaugethedocumentation\u2019s\nquirements. actualvaluetoadeveloper.\n5\n3.3 Truthfulness dict documentation based on surrounding code.\nWeuseCodeLlama-13B(Roziereetal.,2023),an\nA critical dimension of documentation quality is\nopenmodeltrainedwithFIMtasks(Bavarianetal.,\nits factual accuracy, or Truthfulness. Documen-\n2022). AbbreviatedasFIM-CL.Chat: Represents\ntation, especiallywhengeneratedbyLLMsunfa-\ngenerating documentation by providing the code\nmiliar with a specific private codebase, can suf-\nsnippetdirectlytoachat-basedLLM.Wetesttwo\nferfrom\"hallucinations\"\u2014confidentlyreferencing\nleading models: GPT-4o mini 7(OpenAI, 2022)\nnon-existent methods, parameters, or classes, or\nandCodeLlama-34B-instruct(Roziereetal.,2023).\nmisrepresentingrelationshipsbetweencomponents.\nAbbreviatedasChat-GPTandChat-CL,respec-\nSuchinaccuraciesseverelyunderminetrustandcan\ntively.\nmisleaddevelopers.\nWeevaluateTruthfulnessbyverifyingwhether\n4.2 ExperimentSetup\nentities mentioned in the generated documenta-\nData. WeselectarepresentativesubsetofPython\ntionactuallyexistwithinthetargetrepositoryand\nrepositoriestoensurediversityinsize,complexity,\nare referenced correctly. Our pipeline comprises\nanddomain. Thedatasetcomprisesmodules,func-\nthree stages: Code EntityExtraction: An LLM\ntions,methods,andclasseswithvaryingdegreesof\nis prompted to identify mentions of repository-\ndependencydensity(detailsinAppendixD).\nspecificcodecomponents(classes,functions,meth-\nSystems. Weevaluatetwovariantsofourproposed\nods, attributes) within the generated docstring.\nsystem,differingonlyinthebackboneLLMused\nThepromptspecificallyinstructsthemodeltodis-\nbytheagents: DA-GPT:DocAgentutilizingGPT-\ntinguish these from standard language keywords,\n4omini. DA-CL:DocAgentutilizingCodeLlama-\nbuilt-intypes(e.g.,list,dict),andcommonex-\n34B-instruct8.\nternal library components, focusing on internal\nStatistical Significance. All claims of statistical\nreferences. Ground Truth Construction: We\nsignificancearebasedonpairedt-testswithasig-\nleveragethedependencygraphconstructedbythe\nnificancethresholdofp < 0.059\nNavigator module 2.1. This graph serves as the\ngroundtruth,containingacanonicalrepresentation\n4.3 ExperimentResults\nofallcodecomponentsandtheirlocationswithin\nWeevaluatethesystemsusingtheframeworkpro-\ntherepository. Verification: Eachextractedentity\nposed in Section 3, focusing on Completeness,\nmentioniscross-referencedagainstthedependency\nHelpfulness,andTruthfulness.\ngraph.\nWe quantify Truthfulness using the Existence\n4.3.1 Completeness\nRatio: the proportion of unique repository-\nspecific entities mentioned in the documenta-\nSystem Overall Function Method Class\ntion that correspond to actual entities in the DA-GPT 0.934\u2020 0.945\u2020 0.935\u2020 0.914\u2020\ncodebase.ExistenceRatio = |VerifiedEntities| DA-CL 0.953\u2020\u2021 0.985\u2020\u2021 0.982\u2020\u2021 0.816\u2020\u2021\n|ExtractedEntities|\nAhighratioindicatesthatthedocumentationis Chat-GPT 0.815 0.828 0.823 0.773\nwell-grounded in the actual code structure, mini- Chat-CL 0.724 0.726 0.744 0.667\nmizingtheriskofhallucinatedreferences. FIM-CL 0.314 0.291 0.345 0.277\nTogether, these three dimen-\nTable1: AverageCompletenessScores. \u2020: Significantly\nsions\u2014Completeness,Helpfulness,andTruthful-\nbetterthancorrespondingChatbaseline.\u2021:Significantly\nness\u2014providearobustandnuancedframeworkfor\nbetterthanFIMbaseline.\nevaluatingautomaticcodedocumentationsystems,\nenabling quantitative comparisons and deeper As shown in Table 1, both DocAgent variants\ninsightsintotheirstrengthsandweaknesses. significantlyoutperformtheirrespectiveChatcoun-\nterparts. DocAgent(CodeLlama-34B)achievesan\n4 Experiment\n72024-07-18version\n4.1 Baselines 8ThechoiceofbackboneLLMisorthogonaltotheDocA-\ngentframeworkitself.WeuseGPT-4o-2024-08-06universally\nWecompareDocAgentagainsttworepresentative forrunningevaluationformorerobustresults.\nbaseline systems commonly used for code docu- 9Duetospacelimitations,weareunabletoincludethefull\npromptsanddetailedexperimentalsetupinthepaper. How-\nmentation generation: FIM (Fill-in-the-middle):\never,allconfigurationsareavailableinourproject\u2019spublic\nSimulates inline code completion tools that pre- releaserepository.\n6\noverall score of 0.953, representing a substantial System Verified Extracted ExistenceRatio(%)\nimprovementof0.229pointsoverChat. Similarly, DA-GPT 265 305 95.74%\nDA-CL 354 600 88.17%\nDocAgent(GPT-4omini)scores0.934overall,sig-\nChat-GPT 366 347 61.10%\nnificantly higher than Chat at 0.815. These im-\nChat-CL 366 488 68.03%\nprovements are statistically significant across all\nFIM-CL 338 131 45.04%\ncomponenttypes. FIMperformspoorly,achieving\nanoverallcompletenessscoreofonly0.314. This Table 3: Truthfulness Analysis: Existence Ratio (%).\nhighlights the effectiveness of DocAgent\u2019s struc- Higherisbetter. Extracted=extractedentities;Verifed\ntured,context-awaregenerationprocesscompared =verifiedentitiesin\u00a73.3.\ntosimplypromptinganLLMwiththecodeiniso-\nlation.\nmodeloftenleadstoinaccurateassumptionsorhal-\n4.3.2 Helpfulness lucinationsaboutthesurroundingcodebasecontext.\nFIM performs worst, with an Existence Ratio of\nAs shown in Table 2, DocAgent (GPT-4o mini)\nonly45.04%,implyingthatnearlyhalfofitsrefer-\nachievesthehighestoverallhelpfulnessscore,sig-\nencestorepositoryentitiesmightbeincorrect. This\nnificantly outperforming the corresponding Chat\nlowscorehighlightsasignificantriskofmisleading\nbaseline. demonstrating its ability to generate\ndeveloperswhenusingFIMfordocumentation.\nclearerandmoreinformativecontentbyleveraging\nretrievedcontext.\n4.4 AblationStudy\nSystem Overall Summary Description Parameters\nToisolatethecontributionofthedependency-aware\nDA-GPT 3.88\u2020 4.32\u2020 3.60\u2020 2.71\nDA-CL 2.35\u2021 2.36\u2020\u2021 2.43\u2021 2.00 processingorderdeterminedbytheNavigatormod-\nChat-GPT 2.95 3.56 2.42 2.20 ule (\u00a7 2.1), we conducted an ablation study. We\nChat-CL 2.16 2.04 2.37 1.80 createdvariantsofDocAgent(DA-Rand-GPT,DA-\nFIM-CL 1.51 1.30 2.45 1.50\nRand-CL) that process components in a random\nTable2: AverageHelpfulnessScores. \u2020: Significantly\norder10.\nbetterthancorrespondingChat. \u2021: Significantlybetter\nthanFIM. 4.4.1 ImpactonHelpfulness\nDocAgent(CodeLlama-34B)alsoshowsanim-\nSystem Overall Summary Description Parameters\nprovementoveritsChatcounterpart,producingsig- DA-GPT 3.88\u2020 4.32\u2020 3.60 2.71\nDA-Rand-GPT 3.44(-0.44) 3.62(-0.70) 3.30(-0.30) 2.20(-0.51)\nnificantly more helpful summaries. Furthermore,\nDA-CL 2.35\u2020 2.36\u2020 2.43 2.00\nDocAgent(CodeLlama-34B)alsosignificantlyout- DA-Rand-CL 2.18(-0.17) 1.88(-0.48) 2.42(-0.10) 2.00(0.00)\nperforms FIM. Across aspects, generating help-\nTable 4: Ablation: Average Helpfulness Scores. \u2020 If\nfulparameterdescriptionsappearsmostchalleng-\nDocAgentsignificantlybetterthanitsRandomvariant.\ning. DocAgent(GPT-4omini)achievesthehighest\nscoreevenhere,suggestingitsstructuredapproach\nThe results in Table 4 demonstrate the benefit\naids in this difficult task, although room for im-\nof the Navigator\u2019s topological sorting in improv-\nprovementremains.\ning Helpfulness. For both underlying LLMs, the\n4.3.3 Truthfulness fullDocAgentachievedsignificantlyhigheroverall\nThe results in Table 3 demonstrate the superior helpfulness scores compared to its random-order\nfactual accuracy of documentation generated by counterpart. With GPT-4o mini, the full DocA-\nDocAgent. DocAgent(GPT-4omini)achievesthe gentscored3.69overall,significantlyhigherthan\nhighestExistenceRatioat95.74%,indicatingthat DocAgent-Random\u2019s3.44. Theimprovementwas\nthevastmajorityofitsreferencestointernalcode particularly pronounced in summary generation.\ncomponents are correct. DocAgent (CodeLlama- Similarly,withCodeLlama-34B,thefullDocAgent\n34B)alsoperformsstronglywitharatioof88.17%. scored 2.39 overall, significantly outperforming\nThis contrasts sharply with the baselines. The DocAgent-Random\u2019s 2.18. Again, the summary\nChatapproachesexhibitsignificantlylowertruth- scoresshowedasignificantdifference.\nfulness, with Chat (GPT-4o mini) at 61.10% and\n10Completenesswasomittedfromtheablationstudybe-\nChat(CodeLlama-34B)at68.03%. Thissuggests\ncauseitdependsonthecode\u2019sstructure,nottheNavigator\u2019s\nthat simply providing the code snippet to a chat processingorder.\n7\n4.4.2 ImpactonTruthfulness tation. An ablation study confirmed the critical\nWe also evaluated the impact of removing the hi- contribution of the topological processing order\nerachicalgenerationorderonthefactualaccuracy to both helpfulness and truthfulness. DocAgent\n(Truthfulness). WithouttheNavigator,theSearcher represents a promising step towards reliable and\ncanstillretrievedependentcodecomponents. How- usefulautomatedcodedocumentationgeneration\never,sincethe\u2019DependenciesFirst\u2019principleisnot forcomplexandproprietarysoftware.\nfollowed,thesecomponentsarelesslikelytohave\n6 EthicsandLimitations\nalreadygenerateddocumentationavailableforcon-\ntext. DocAgent,whileadvancingautomatedcodedoc-\numentation, has inherent limitations and ethical\nSystem Verified Extracted ExistenceRatio(%)\nconsiderations. Technically,processingextremely\nDA-GPT 187 224 94.64%\nDA-Rand-GPT 164(-23) 166(-58) 86.75(-7.89)% largecodebasesmaystillchallengeLLMcontext\nDA-CL 190 343 87.76% limitsdespitetopologicalsortingandcontextman-\nDA-Rand-CL 188(-2) 360(+17) 83.06(-4.70)%\nagement. Relyingsolelyonstaticanalysisrestricts\nunderstandingofdynamicbehavior,andthecurrent\nTable 5: Ablation: Truthfulness Analysis (Existence\nRatio%). Use50randomlysampledcodecomponents Pythonfocusrequireseffortforadaptationtoother\nfromfulldatatoevaluate. languages.\nEthically, the primary concern is factual accu-\nTable 5 demonstrates that the topological sort\nracy;generateddocumentation,thoughimproved,\nalso improves truthfulness. Both full DocAgent\nmaystillcontainhallucinationsorinaccuracies,po-\nvariantsachievehigherExistenceRatiosthantheir\ntentially misleading developers. The underlying\nrandom-order counterparts. Existence ratio of\nLLMs may propagate biases from their training\nDocAgent (GPT-4o-mini) drops from 94.64% to\ndataintothedocumentation. Over-relianceonsuch\n86.75%withoutthesort,andtheratioofDocAgent\ntools could potentially hinder developers\u2019 deep\n(Codellama-34B)dropsfrom87.76%to83.06%.\ncode comprehension skills. Applying DocAgent\nCollectively,theablationresultsconfirmthatthe\nto proprietary code necessitates careful handling,\nNavigator\u2019sdependency-awaretopologicalorder-\nespeciallyregardingexternalqueries,toavoidinad-\ning is a crucial component of DocAgent, signif-\nvertentlyleakingsensitiveinformation. Finally,the\nicantly contributing to both the helpfulness and\ncomputationalresourcesrequiredforLLM-driven\nfactual accuracy of the generated documentation\nmulti-agent systems represent a notable cost and\nbyenablingeffectiveincrementalcontextmanage-\nenvironmentalconsideration. Futureworkshould\nment.\naddresstheselimitations,focusingonrobustness,\n5 Conclusion biasmitigation,anddeeperevaluation,whileem-\nphasizingthathumanoversightremainscrucialin\nWeaddressedthechallengeofautomaticallygen-\npracticaldeployment.\nerating high-quality code documentation, a task\nwhere existing LLM-based methods often strug-\nglewithincompleteness,lackofhelpfulness,and References\nfactualinaccuracies. WeintroducedDocAgent,a\nSamuelAbedu,AhmadAbdellatif,andEmadShihab.\nnoveltool-integrated,multi-agentsystemthatlever- 2024. Llm-basedchatbotsforminingsoftwarerepos-\nages a dependency-aware topological processing itories:Challengesandopportunities. InProceedings\norderdeterminedbyaNavigatormodule. Thisal- ofthe28thInternationalConferenceonEvaluation\nandAssessmentinSoftwareEngineering,pages201\u2013\nlowsspecializedagents(Reader,Searcher,Writer,\n210.\nVerifier,Orchestrator)tocollaborativelygenerate\ndocumentationbyincrementallybuildingcontext Emad Aghajani, Csaba Nagy, Olga Lucero Vega-\nM\u00e1rquez, Mario Linares-V\u00e1squez, Laura Moreno,\nfrom dependencies. We also proposed a robust\nGabrieleBavota,andMicheleLanza.2019. Software\nandscalableevaluationframeworkassessingCom-\ndocumentationissuesunveiled. In2019IEEE/ACM\npleteness, Helpfulness, and Truthfulness. Our 41stInternationalConferenceonSoftwareEngineer-\nexperimentsondiversePythonrepositoriesdemon- ing(ICSE),pages1199\u20131210.IEEE.\nstratethatDocAgentsignificantlyoutperformsFIM\nWasi U Ahmad, Saikat Chakraborty, Baishakhi Ray,\nand Chat baselines consistently, producing more\nandKai-WeiChang.2021. Unifiedpre-trainingfor\ncomplete,helpful,andfactuallyaccuratedocumen- programunderstandingandgeneration. InACL.\n8\nAnthropic.2025. Modelcontextlengthincreaseswith Liron Guelman, Alon Lavie, and Eran Yahav. 2024.\nthenewcontextprotocol. Accessed: 2025-03-27. Usinglargelanguagemodelstodocumentcode: A\nfirstquantitativeandqualitativeassessment. arXiv\nMohammad Bavarian, Heewoo Jun, Nikolas Tezak, preprintarXiv:2403.04264.\nJohnSchulman,ChristineMcLeavey,JerryTworek,\nand Mark Chen. 2022. Efficient training of lan- DayaGuo,ShuoRen,ShuaiLu,ZhangyinFeng,Duyu\nguage models to fill in the middle. arXiv preprint Tang,NanDuan,MingZhou,andDaxinJiang.2021.\narXiv:2207.14255. Graphcodebert: Pre-training code representations\nwithdataflow. InICLR.\nJie-CherngChenandSun-JenHuang.2009. Anempiri-\nSeungone Kim, Soobin Kim, Alice Oh, and Gunhee\ncalanalysisoftheimpactofsoftwaredevelopment\nHan.2023. Prometheus: Inducingfine-grainedeval-\nproblemfactorsonsoftwaremaintainability. Journal\nuationcapabilityinlanguagemodels. arXivpreprint\nofSystemsandSoftware,82(6):981\u2013992.\narXiv:2310.08491.\nMarkChen,JerryTworek,HeewooJun,QimingYuan,\nMichaelKrumdick,JasonWei,XinyangChen,Shang-\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nbin Du, Shu Xu, Dale Schuurmans, and Ed H Chi.\nHarriEdwards,YuriBurda,NicholasJoseph,Greg\n2025. No free labels: Limitations of llm-as-a-\nBrockman, and 1 others. 2021. Evaluating large\njudge without human grounding. arXiv preprint\nlanguage models trained on code. arXiv preprint\narXiv:2503.05061.\narXiv:2107.03374.\nYukyungLee,WonjoonCho,andJinhyukKim.2024.\nQianChen,BinyuanTang,YankaiZhang,BinhuaWang, Checkeval: Areliablellm-as-a-judgeframeworkfor\nZhifang Zhang, and Qun Zhang. 2023. Teaching evaluating text generation using checklists. arXiv\nlargelanguagemodelstoself-debug. arXivpreprint preprintarXiv:2403.18771.\narXiv:2305.03047.\nRaymondLi,LewisTunstall,PatrickvonPlaten,Jung-\nCheng-HanChiangandHung-yiLee.2023. Canlarge taekKim,TevenLeScao,ThomasWolf,andAlexan-\nlanguagemodelsbeanalternativetohumanevalua- derM.Rush.2023a. Starcoder: Maythesourcebe\ntions? InProceedingsofthe61stAnnualMeetingof withyou! Preprint,arXiv:2305.06161.\ntheAssociationforComputationalLinguistics(ACL).\nXiang Li, Qinyuan Zhu, Yelong Cheng, Weizhu\nColinBClement,AndrewTerrell,HanlinMao,Joshua Xu, and Xi Liu. 2023b. Camel: Communica-\nDillon, Sameer Singh, and Dan Alistarh. 2020. tive agents for \u201cmind\u201d exploration. arXiv preprint\nPymt5: Multi-modetranslationofnaturallanguage arXiv:2303.17760.\nandpythoncodewithtransformers. InEMNLP.\nMinqian Liu, Cheng Feng, Qing Lyu, Wenhao Zeng,\nSergio Cozzetti B De Souza, Nicolas Anquetil, and Chao Zheng, Ruidan Zhang, and Steven C H Lin.\nK\u00e1thia M de Oliveira. 2005. A study of the doc- 2023a. X-eval: Generalizable multi-aspect text\numentation essential to software maintenance. In evaluationviaaugmentedinstructiontuning. arXiv\nProceedingsofthe23rdannualinternationalconfer- preprintarXiv:2311.08788.\nenceonDesignofcommunication: documenting&\nYang Liu, Yao Fu, Yujie Xie, Xinyi Chen, Bo Pang,\ndesigningforpervasiveinformation,pages68\u201375.\nChenyan Qian, Teng Ma, and Dragomir Radev.\n2023b. G-eval: Nlgevaluationusinggpt-4withbet-\nShubhang Shekhar Dvivedi, Vyshnav Vijay, Sai\nter human alignment. In Proceedings of the 2023\nLeelaRahulPujari,ShoumikLodh,andDhruvKu-\nConference on Empirical Methods in Natural Lan-\nmar.2024. Acomparativeanalysisoflargelanguage\nguageProcessing.\nmodelsforcodedocumentationgeneration. InPro-\nceedingsofthe1stACMInternationalConferenceon\nQinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang,\nAI-PoweredSoftware,pages65\u201373.\nYujiaQin,YaxiLu,YesaiWu,XinCong,YankaiLin,\nYingli Zhang, and 1 others. 2024. Repoagent: An\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,\nllm-poweredopen-sourceframeworkforrepository-\nXiaocheng Feng, Ming Gong, Linjun Shou, Bing\nlevelcodedocumentationgeneration. arXivpreprint\nQin, Ting Liu, and Daxin Jiang. 2020. Codebert:\narXiv:2402.16667.\nA pre-trained model for programming and natural\nlanguages. InEMNLP. YingweiMa,QingpingYang,RongyuCao,BinhuaLi,\nFei Huang, and Yongbin Li. 2024. How to under-\nGolara Garousi, Vahid Garousi-Yusifog\u02d8lu, Guenther stand whole software repository? arXiv preprint\nRuhe, Junji Zhi, Mahmoud Moussavi, and Brian arXiv:2406.01422.\nSmith. 2015. Usage and usefulness of technical\nsoftware documentation: An industrial case study. PaulWMcBurney,SiyuanJiang,MarouaneKessentini,\nInformationandsoftwaretechnology,57:664\u2013682. NicholasAKraft,AmeerArmaly,MohamedWiem\nMkaouer,andCollinMcMillan.2017. Towardspri-\nGitHub.2024. Howgithubcopilotisgettingbetterat oritizingdocumentationeffort. IEEETransactions\nunderstandingyourcode. Accessed: 2025-03-27. onSoftwareEngineering,44(9):897\u2013913.\n9\nMeta. 2025. Meta ai. https://ai.meta.com/ models for code understanding and generation. In\nmeta-ai/. Accessed: 2025-03-27. EMNLP.\nOpenAI.2022. Introducingchatgpt. Accessed: 2025- Ziniu Wu, Cheng Liu, Jindong Zhang, Xinyun Li,\n03-27. YewenWang,JimmyXin,LianminZhang,EricXing,\nYuxinLu,andPercyLiang.2023. Autogen:Enabling\nDavidLorgeParnas.2010. Precisedocumentation: The\nnext-generationmulti-agentcommunicationwithlan-\nkey to better software. In The future of software\nguagemodels. arXivpreprintarXiv:2309.07864.\nengineering,pages125\u2013148.Springer.\nDayuYang,TianyangLiu,DaoanZhang,and1others.\nYuzhang Qian, Zian Zhang, Liang Pan, Peng Wang,\n2025. Codetothink,thinktocode:Asurveyoncode-\nShouyi Liu, Wayne Xin Zhao, and Ji-Rong Wen.\nenhancedreasoningandreasoning-drivencodeintel-\n2023. Chatdev: Revolutionizingsoftwaredevelop-\nligenceinllms. arXivpreprintarXiv:2502.19411.\nment with ai-collaborative agents. arXiv preprint\narXiv:2307.07924.\nGuang Yang, Yu Zhou, Wei Cheng, Xiangyu Zhang,\nXiang Chen, Terry Yue Zhuo, Ke Liu, Xin Zhou,\nRafaelRafailov,ArchitSharma,EricMitchell,Christo-\nDavid Lo, and Taolue Chen. 2024. Less is more:\npherDManning,StefanoErmon,andChelseaFinn.\nDocstring compression in code generation. arXiv\n2023. Direct preference optimization: Your lan-\npreprintarXiv:2410.22793.\nguagemodelissecretlyarewardmodel. Advancesin\nNeuralInformationProcessingSystems,36:53728\u2013\nShinnYao,JeffreyZhao,DianYu,KangChen,Karthik\n53741.\nNarasimhan,andYuanCao.2022. React: Synergiz-\nMartin P Robillard. 2009. What makes apis hard to ingreasoningandactinginlanguagemodels. arXiv\nlearn? answers from developers. IEEE software, preprintarXiv:2210.03629.\n26(6):27\u201334.\nYaqing Zan, Mingyu Ding, Bill Yuchen Lin, and Xi-\nRahul Roy, Saikat Chakraborty, Baishakhi Ray, and angRen.2022. Whenlanguagemodelmeetsprivate\nMiryungKim.2021. Reassessingautomaticevalu- library. InProceedingsofthe2022Conferenceon\nationmetricsforcodesummarizationtasks. InPro- EmpiricalMethodsinNaturalLanguageProcessing\nceedingsofthe29thACMJointEuropeanSoftware (EMNLP). Association for Computational Linguis-\nEngineeringConferenceandSymposiumontheFoun- tics.\ndationsofSoftwareEngineering(ESEC/FSE),pages\n1344\u20131356. Kaiyu Zhang, Yifei Wang, Yue Yu, Yujie Li, Zihan\nLin, Dongxu Zhang, Yichi Zhou, Yifei Xu, Ang\nBaptisteRoziere,JonasGehring,FabianGloeckle,Sten Chen, Weiyi Zhang, and 1 others. 2024. Llm hal-\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, lucinations in practical code generation: Phenom-\nJingyu Liu, Romain Sauvestre, Tal Remez, and 1 ena, mechanism, and mitigation. arXiv preprint\nothers.2023. Codellama: Openfoundationmodels arXiv:2401.10650.\nforcode. arXivpreprintarXiv:2308.12950.\nShiyueZhang,BinyiLi,JasonWei,AditiRaghunathan\nNoah Shinn, Margaret Labash, and Stefano Er-\nVyas, and Percy Liang. 2023a. Themis: A flexi-\nmon. 2023. Reflexion: Language agents with\nble and interpretable nlg evaluation model. arXiv\nverbal reinforcement learning. arXiv preprint\npreprintarXiv:2309.12082.\narXiv:2303.11366.\nXiaoqingZhang,ZhiruiWang,LichaoYang,WeiZhang,\nRobertTarjan.1972. Depth-firstsearchandlineargraph\nand Yong Zhang. 2023b. Mapcoder: Map-reduce-\nalgorithms. SIAMjournaloncomputing,1(2):146\u2013\nstylecodegenerationwithmulti-agentcollaboration.\n160.\narXivpreprintarXiv:2307.15808.\nGiasUddin,FoutseKhomh,andChanchalKRoy.2021.\nLianminZheng,ShangbinDu,YuhuiLin,YukuoShao,\nAutomatic api usage scenario documentation from\nZiLin,ZhenLiu,and1others.2023. Judgingllm-\ntechnicalq&asites. ACMTransactionsonSoftware\nas-a-judgewithmt-benchandchatbotarena. arXiv\nEngineering and Methodology (TOSEM), 30(3):1\u2013\npreprintarXiv:2306.05685.\n45.\nYidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, ZihanZheng,JiayiZheng,WeiyanLiu,YizhongWang,\nXin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, ChenLiu,XiangLorraineLi,MuLi,WenhaoZhang,\nQingsongWen,WeiYe,and1others.2024. Autosur- Diyi Huang, and Xiang Ren. 2024. How well do\nvey: Largelanguagemodelscanautomaticallywrite llmsgeneratecodefordifferentapplicationdomains?\nsurveys. AdvancesinNeuralInformationProcessing arXivpreprintarXiv:2401.13727.\nSystems,37:115119\u2013115145.\nShuyan Zhou, Uri Alon, Frank F Xu, Zhiruo\nYue Wang, Shuo Ren, Daya Lu, Duyu Tang, Nan Wang,ZhengbaoJiang,andGrahamNeubig.2022.\nDuan,MingZhou,andDaxinJiang.2021. Codet5: Docprompting: Generating code by retrieving the\nIdentifier-awareunifiedpre-trainedencoder-decoder docs. arXivpreprintarXiv: 2207.05987.\n10\nMingchen Zhuge, Changsheng Zhao, Dylan Ashley,\nWenyi Wang, Dmitrii Khizbullin, Yunyang Xiong,\nZechunLiu,ErnieChang,RaghuramanKrishnamoor-\nthi,YuandongTian,and1others.2024. Agent-as-a-\njudge: Evaluateagentswithagents. arXivpreprint\narXiv:2410.10934.\n11\nA RelatedWork C ScarcityofCodeDocumentation\nWe analyzed 164 top-starred Python repositories\nLLMAgent RecentadvancementsinLLMagents\n(created after January 1, 2025), encompassing\nhave enabled automating complex code-related\n13,314filesand115,943documentablenodes(func-\ntasks (Yang et al., 2025). Single-agent frame-\ntions,classes,andmethods). Ofthesenodes,only\nworks like ReAct (Yao et al., 2022) and Reflex-\n27.28%containedanydocumentation,with66.46%\nion(Shinnetal.,2023)integrateaction-reasoning\nofrepositoriesexhibitinglessthan30%coverage\nandself-reflection. Multi-agentsystems(CAMEL\n(Figure5). Furthermore,62.25%ofrepositoriesav-\n(Lietal.,2023b),AutoGen(Wuetal.,2023))ex-\neraged30wordsorfewerperdocumentationblock\ntend these ideas with role-specialized LLMs and\n(Figure6),whileonly3.98%exceededanaverage\nstructuredcommunicationtohandlemorecomplex\nof 100 words, illustrating the widespread brevity\nproblems. In software development, MapCoder\nandoverallscarcityofcodedocumentation.\n(Zhang et al., 2023b), RGD (Chen et al., 2023),\nandChatDev(Qianetal.,2023)leveragespecial-\nizedagentsformanydownstreamtasks,achieving\nstate-of-the-artcodegeneration. Theseinsightson\nmulti-agentcoordinationandworkflowstructuring\nunderpinourDocAgentframework,whichadopts\natopologically-aware,tool-integratedmulti-agent\ndesign.\nCode Summarization Pre-trained encoders\nsuchasCodeBERT(Fengetal.,2020)andGraph- Figure5: Distributionofrepositoriesbycode\nCodeBERT (Guo et al., 2021) introduced bi- documentationcoverage.\nmodalandstructure-awarelearning,whileencoder-\ndecodermodelsPLBART(Ahmadetal.,2021)and\nCodeT5 (Wang et al., 2021) unified code genera-\ntion and summarization. PyMT5 (Clement et al.,\n2020) extended T5 for Python docstring transla-\ntion with multi-mode support. Recently, LLMs\n(OpenAICodex(Chenetal.,2021),StarCoder(Li\net al., 2023a), CodeLlama (Roziere et al., 2023))\nhave demonstrated strong zero-shot summariza- Figure6: Distributionofrepositoriesbyaver-\nagewordsperdocumentation.\ntion. However, they often lack repository-level\ncontext, dependency awareness, and collabora-\nD Data\ntion\u2014limitations our multi-agent, context-aware\nDOCAGENTaimstoovercome. We gathered 164 top-stared Python repositories\nfromGitHub, eachcreatedafterJanuary1, 2025,\nhavingmorethan50stars,andexceeding50KBin\nB WhyExternalInformationisneeded\nsize. Fromthiscorpus,weselected9repositories\nThe external open-internet information request reflectingtheoveralldistributionintermsoflines\nis necessary for writing documentation for some ofcodeandtopologicalcomplexity. Figure7shows\nnovel,newly-proposedideas,likenovelevaluation the selected repositories (red points) overlaid on\nmethod,algorithm,modelstructure,lossfunctions. thebroaderdistribution. Eventually,wecollected\nFor example, DPO (Rafailov et al., 2023) is a re- 366 code components (120 functions, 178 meth-\ninforcementlearningalgorithmproposedin2023. ods,and68classes)forevaluation,withaseparate\nCodellamahastheknowledgecutoffinSep2022. subsetof50distinctcodecomponents(randomly\nSowhenusingcodellamafordocumentationgen- sampledfromthefullset)usedspecificallyforour\neration,withoutaccessingmathematicalintuition truthfulnessablationstudy.\nand description of DPO from the open internet,\nE RobustLLM-as-judge\ncodellama will not possible to write helpful doc-\numentation that describe the intuition behind the AssessingthequalitativeaspectsofHelpfulnessau-\nimplementationofDPO. tomaticallyisinherentlychallengingduetosubjec-\n12\nThis structured LLM-as-judge approach aims to\nprovideascalableyetnuancedassessmentofthe\ndocumentation\u2019spracticalvaluetodevelopers.\nF MoreSystemScreenshots\nFigure8showstheconfigurationpagebeforeiniti-\natingthecodedocumentationgenerationprocess.\nThe page mainly consists of three parts: the tar-\ngetrepositorypath,LLMconfiguration,andflow\ncontrol(fortheorchestrator).\nFigure7: Distributionofrepositoriesusedfordocstring\ngenerationevaluation.\ntivity. WeemployanLLM-as-judgeapproach,but\nincorporate rigorous mechanisms inspired by ex-\nistingworktoenhancereliabilityandconsistency,\nmitigatingknownissueslikepositionalbiasorvari-\nability(Wangetal.,2024;Zhugeetal.,2024): De-\ncomposed Evaluation: Instead of a single holis-\nticjudgment, theLLMevaluatesdistinctpartsof\nthe docstring (e.g., summary, parameter descrip-\ntions,overalldescription)separately,usingtailored\npromptsforeachpart(Liuetal.,2023a;Leeetal.,\n2024). StructuredPrompting: Eachpromptpro-\nvidestheLLMwith:\n\u2022 ExplicitRubrics: Detailedcriteriadefiningex-\npectationsfordifferentlevelsona5-pointLik-\nertscale(1=Poorto5=Excellent)concerning\nFigure8: Screenshotoftheconfigurationpage.\nclarity, detail, and utility specific to the doc-\nstringpartbeingevaluated(Kimetal.,2023; Figure 9 displays the window that appears af-\nZhangetal.,2023a). ter clicking the \"Evaluate\" button. Since using\nanLLMasajudgeiscostly(consumingapproxi-\n\u2022 Illustrative Examples: Few-shot examples\nmately 500 tokens per evaluation), this feature is\ndemonstrating good and bad documentation\noptionalinthewebUI.Onlywhentheuserclicks\nsnippetscorrespondingtodifferentscorelev-\nthe \"Evaluate\" button will the evaluation be trig-\nels,groundingtherubriccriteria(Zhengetal.,\ngered,afterwhichthebuttonchangestodisplaythe\n2023;ChiangandLee,2023).\ngeneratedscore.\n\u2022 Chain-of-ThoughtInstructions: Guidingthe\nLLMtofirstanalyzethecode,thencompare\nthe corresponding docstring section against\ntherubric,justifyitsratingstep-by-step,and\nidentifyspecificstrengthsorweaknesses(Liu\netal.,2023b;Zhengetal.,2023).\n\u2022 Standardized Output Format: Requiring the\nLLM to output its rating along with de-\ntailedjustificationsinastructuredformat(e.g.,\nFigure9: Screenshotofthehelpfulnessevaluationwin-\nJSON),facilitatingaggregationandanalysis\ndow.\nwhileensuringtheLLMadherestotheeval-\nuationprotocol(Liuetal.,2023b;Leeetal.,\n2024;Krumdicketal.,2025).\n13\n"
}