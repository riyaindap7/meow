name: Daily MoE Scraper

# Run daily at 2 AM IST (8:30 PM UTC previous day)
on:
  schedule:
    - cron: '30 20 * * *'  # 2:00 AM IST = 8:30 PM UTC
  
  # Allow manual trigger
  workflow_dispatch:

jobs:
  scrape-moe:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Free up disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/share/boost
          df -h

      - name: Install Rclone
        run: |
          curl https://rclone.org/install.sh | sudo bash
      
      - name: Configure Rclone for Google Drive
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf << EOF
          [gdrive]
          type = drive
          scope = drive
          service_account_file = /tmp/service-account-key.json
          root_folder_id = https://drive.google.com/drive/folders/1nYUczTuBjUoaSa9cucpjQU8zkEojdHBp?usp=sharing
          EOF
          echo '${{ secrets.GOOGLE_DRIVE_SERVICE_ACCOUNT_JSON }}' > /tmp/service-account-key.json
      
      - name: Download only metadata files
        run: |
          mkdir -p data/moe/metadata
          # Only download JSON/CSV metadata files, not PDFs
          rclone copy gdrive:metadata data/moe/metadata -v --drive-shared-with-me --include "*.json" --include "*.csv"

      
      - name: Install Python dependencies
        run: |
          cd backend
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run MoE scraper (scrape + upload directly to Drive)
        env:
          RCLONE_REMOTE:gdrive:
        run: |
          python backend/services/moe_scraper_service.py --all
      
      - name: Upload updated metadata
        run: |
          rclone copy data/moe/metadata gdrive:metadata -v --drive-shared-with-me
      
      - name: Upload scrape report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-report-${{ github.run_number }}
          path: backend/scrape_report_*.json
          retention-days: 30
      
      - name: Cleanup credentials
        if: always()
        run: |
          rm -f /tmp/service-account-key.json
      
      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'ðŸš¨ Daily MoE Scraper Failed',
              body: 'The daily MoE scraper workflow failed. Check the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details.',
              labels: ['scraper', 'automated']
            })